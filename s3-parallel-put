#!/usr/bin/env python
# Parallel uploads to Amazon AWS S3
#
# The MIT License (MIT)
#
# Copyright (c) 2011-2014 Tom Payne
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO
from gzip import GzipFile
from itertools import chain, imap, islice
import logging
from multiprocessing import JoinableQueue, Process, current_process
from optparse import OptionGroup, OptionParser
import os.path
import re
from ssl import SSLError
import sys
import tarfile
import time
import mimetypes
import stat
import json
import hashlib

import boto3


boto3.set_stream_logger('boto3.resources', logging.ERROR)
DONE_RE = re.compile(r'\AINFO:s3-parallel-put\[putter-\d+\]:\S+\s+->\s+(\S+)\s*\Z')

# These content types are amenable to compression
# WISHLIST more types means more internets
GZIP_CONTENT_TYPES = (
    'application/javascript',
    'application/x-javascript',
    'text/css',
    'text/html',
    'text/javascript',
)
GZIP_ALL = 'all'
# Borrowed this from boto 2.40.0
CannedACLStrings = ['private', 'public-read',
                    'public-read-write', 'authenticated-read',
                    'bucket-owner-read', 'bucket-owner-full-control',
                    'log-delivery-write']
DEFAULT_CANNED_ACL = 5
GIT_IGNORE = '.gitignore'
SVG_CONTENT_TYPE = 'image/svg+xml'
EOT_CONTENT_TYPE = 'application/vnd.ms-fontobject'
WOFF_CONTENT_TYPE = 'font/woff'
WOFF2_CONTENT_TYPE = 'font/woff2'
DEFAULT_CONTENT_TYPE = 'text/plain'


def repeatedly(func, *args, **kwargs):
    while True:
        yield func(*args, **kwargs)


class FileObjectCache(object):

    def __init__(self):
        self.name = None
        self.file_object = None

    def open(self, name, *args):
        if name != self.name:
            self.name = name
            self.file_object = open(self.name, *args)
        return self

    def __enter__(self):
        return self.file_object

    def __exit__(self, exc_type, exc_value, traceback):
        pass


class Value(object):

    def __init__(self, file_object_cache, content=None, filename=None, md5=None, offset=None, path=None, size=None, bucket_name=None):
        self.file_object_cache = file_object_cache
        self.content = content
        self.filename = filename
        self.md5 = md5
        self.offset = offset
        self.path = path
        self.size = size
        self.bucket_name = bucket_name

    def get_content(self):
        if self.content is None:
            if self.filename:
                with self.file_object_cache.open(self.filename) as file_object:
                    file_object.seek(self.offset)
                    self.content = file_object.read(self.size)
            elif self.path:
                with open(self.path, 'rb') as file_object:
                    self.content = file_object.read()
            else:
                assert False
        return self.content

    def calculate_md5(self):
        if self.md5 is None:
            self.md5 = hashlib.md5(StringIO(self.get_content()).getvalue()).hexdigest()
        return self.md5

    def get_size(self):
        if self.size is None:
            if self.content:
                self.size = len(self.content)
            elif self.path:
                self.size = os.stat(self.path).st_size
            else:
                assert False
        return self.size

    def should_copy_content(self):
        return self.bucket_name is None


def walk_filesystem(source, options):
    if os.path.isdir(source):
        for dirpath, dirnames, filenames in os.walk(source):
            for filename in filenames:
                path = os.path.join(dirpath, filename)
                if not os.path.isfile(path):
                    continue
                key_name = os.path.normpath(os.path.join(options.prefix, path))
                yield (key_name, dict(path=path))
    elif os.path.isfile(source):
        key_name = os.path.normpath(os.path.join(options.prefix, source))
        yield (key_name, dict(path=source))


def walk_tar(source, options):
    try:
        tar_file = tarfile.open(source, 'r:')
        for tarinfo in tar_file:
            if tarinfo.isfile():
                path = tarinfo.name
                key_name = os.path.normpath(os.path.join(options.prefix, path))
                filename = source
                offset = tarinfo.offset_data
                size = tarinfo.size
                yield (key_name, dict(filename=filename, offset=offset, path=path, size=size))
            # http://blogs.oucs.ox.ac.uk/inapickle/2011/06/20/high-memory-usage-when-using-pythons-tarfile-module/
            tar_file.members = []
    except tarfile.ReadError:
        tar_file = tarfile.open(source)
        for tarinfo in tar_file:
            if tarinfo.isfile():
                path = tarinfo.name
                key_name = os.path.normpath(os.path.join(options.prefix, path))
                content = tar_file.extractfile(tarinfo).read()
                yield (key_name, dict(content=content, path=path))


#def walk_s3(source, options):
    # TODO: Not addressing this function for boto3 port for first pass...
    # connection = S3Connection(host=options.host, is_secure=options.secure)
    #for key in connection.get_bucket(source).list():
    #    yield (
    #        key.name,
    #        dict(
    #            bucket_name=key.bucket.name,
    #            md5=key.etag,
    #            size=key.size,
    #            path='%s/%s' % (source, key.name)))


def walker(walk, put_queue, sources, options):
    logger = logging.getLogger('%s[walker-%d]' % (os.path.basename(sys.argv[0]), current_process().pid))
    pairs = chain(*imap(lambda source: walk(source, options), sources))
    if options.resume:
        done = set()
        for filename in options.resume:
            with open(filename) as file_object:
                for line in file_object:
                    match = DONE_RE.match(line)
                    if match:
                        done.add(match.group(1))
        pairs = ((key_name, args) for key_name, args in pairs if key_name not in done)
    if options.limit:
        pairs = islice(pairs, options.limit)
    for pair in pairs:
        put_queue.put(pair)


def put_add(bucket, key_name, value):
    key = bucket.get_key(key_name)
    if key is None:
        return bucket.new_key(key_name)
    else:
        return None


def put_stupid(bucket, key_name, value):
    return key_name


def put_update(bucket, key_name, value):
    key = None
    for s3object in bucket.objects.all():
        if s3object.key == key_name:
            existing_object = s3object
            key = existing_object.key
    if key is None:
        return key_name
    else:
        value.calculate_md5()
        if existing_object.e_tag == '"%s"' % value.md5:
            return None
        else:
            return key


def put_copy(bucket, key_name, value):
    return bucket.copy_key(key_name, value.bucket_name, key_name)


def create_attr(value, options):
    attr = {}
    now = time.mktime(time.gmtime())
    attr['st_uid'] = options.yas3fs_uid
    attr['st_gid'] = options.yas3fs_gid
    attr['st_mode'] = int(stat.S_IFREG | 0755)
    attr['st_atime'] = now
    attr['st_mtime'] = now
    attr['st_ctime'] = now
    attr['st_size'] = value.get_size()
    attr['st_nlink'] = 1
    return attr


def putter(put, put_queue, stat_queue, options):
    logger = logging.getLogger('%s[putter-%d]' % (os.path.basename(sys.argv[0]), current_process().pid))
    s3_connection, bucket = None, None
    file_object_cache = FileObjectCache()
    # Figure out what content types we want to gzip
    if not options.gzip_type:  # default
        gzip_content_types = GZIP_CONTENT_TYPES
    elif 'all' in options.gzip_type:
        gzip_content_types = GZIP_ALL
    else:
        gzip_content_types = options.gzip_type
    if 'guess' in gzip_content_types:
        # don't bother removing 'guess' from the list since nothing will match it
        gzip_content_types.extend(GZIP_CONTENT_TYPES)
    if options.gzip:
        logger.debug('These content types will be gzipped: %s' % unicode(gzip_content_types))
    while True:
        args = put_queue.get()
        if args is None:
            put_queue.task_done()
            break
        key_name, value_kwargs = args
        value = Value(file_object_cache, **value_kwargs)
        should_gzip = False
        try:
            if s3_connection is None:
                s3_connection = boto3.resource('s3')
            if bucket is None:
                bucket = s3_connection.Bucket(options.bucket)
            key = put(bucket, key_name, value)
            if key and GIT_IGNORE not in key:
                metadata = {}
                if value.should_copy_content():
                    if options.headers:
                        metadata = dict(tuple(header.split(':', 1)) for header in options.headers)
                    content_type = None
                    content_encoding = None
                    if options.content_type:
                        if options.content_type == "guess":
                            content_type = mimetypes.guess_type(value.path)[0]
                        else:
                            content_type = options.content_type
                    content = value.get_content()
                    should_gzip = options.gzip and (
                        content_type and content_type in gzip_content_types or
                        gzip_content_types == GZIP_ALL)
                    if should_gzip:
                        content_encoding = 'gzip'
                        string_io = StringIO()
                        gzip_file = GzipFile(compresslevel=9, fileobj=string_io, mode='w')
                        gzip_file.write(content)
                        gzip_file.close()
                        content = string_io.getvalue()
                    if options.yas3fs:
                        attr = create_attr(value, options)
                        logger.debug('yas3fs attributes ' + json.dumps(attr))
                        metadata['attr'] = json.dumps(attr)
                    if content_type is None:
                        filename, file_extension = os.path.splitext(key)
                        del filename
                        if file_extension == '.svg' or file_extension == '.gz':
                            content_type = SVG_CONTENT_TYPE
                        elif file_extension == '.eot':
                            content_type = EOT_CONTENT_TYPE
                        elif file_extension == '.woff':
                            content_type = WOFF_CONTENT_TYPE
                        elif file_extension == '.woff2':
                            content_type = WOFF2_CONTENT_TYPE
                        else:
                            content_type = DEFAULT_CONTENT_TYPE
                    if not options.dry_run:
                        new_obj = bucket.put_object(
                            Key=key,
                            Body=content,
                            ContentType=content_type,
                            Metadata=metadata,
                            ACL=options.grant,
                        )
                        if content_encoding:
                            new_obj.content_encoding = content_encoding
                        # TODO: Figure out how to re-implement the "encrypt_key" parameter from boto 2's "set_contents__from_string".
                logger.info('%s %s> %s' % (value.path, 'z' if should_gzip else '-', key))
                stat_queue.put(dict(size=value.get_size()))
            else:
                logger.info('skipping %s -> %s' % (value.path, key_name))
        except SSLError as exc:
            logger.error('%s -> %s (%s)' % (value.path, key_name, exc))
            put_queue.put(args)
            connection, bucket = None, None
        except IOError as exc:
            logger.error('%s -> %s (%s)' % (value.path, key_name, exc))
        put_queue.task_done()


def statter(stat_queue, start, options):
    logger = logging.getLogger('%s[statter-%d]' % (os.path.basename(sys.argv[0]), current_process().pid))
    count, total_size = 0, 0
    while True:
        kwargs = stat_queue.get()
        if kwargs is None:
            stat_queue.task_done()
            break
        count += 1
        total_size += kwargs.get('size', 0)
        stat_queue.task_done()
    duration = time.time() - start
    logger.info('put %d bytes in %d files in %.1f seconds (%d bytes/s, %.1f files/s)' % (total_size, count, duration, total_size / duration, count / duration))


def main(argv):
    parser = OptionParser()
    group = OptionGroup(parser, 'S3 options')
    group.add_option('--bucket', metavar='BUCKET',
            help='set bucket')
    group.add_option('--bucket_region', default='us-east-1',
            help='set bucket region if not in us-east-1 (default new bucket region)')
    group.add_option('--host', default='s3.amazonaws.com',
            help='set AWS host name')
    group.add_option('--insecure', action='store_false', dest='secure',
            help='use insecure connection')
    group.add_option('--secure', action='store_true', default=True, dest='secure',
            help='use secure connection')
    parser.add_option_group(group)
    group = OptionGroup(parser, 'Source options')
    group.add_option('--walk', choices=('filesystem', 'tar', 's3'), default='filesystem', metavar='MODE',
            help='set walk mode (filesystem or tar)')
    parser.add_option_group(group)
    group = OptionGroup(parser, 'Put options')
    group.add_option('--content-type', default='guess', metavar='CONTENT-TYPE',
            help='set content type, set to "guess" to guess based on file name')
    group.add_option('--gzip', action='store_true',
            help='gzip values and set content encoding')
    group.add_option('--gzip-type', action='append', default=[],
            help='if --gzip is set, sets what content-type to gzip, defaults '
            'to a list of known text content types, "all" will gzip everything.'
            ' Specify multiple times for multiple content types. '
            '[default: "guess"]')
    group.add_option('--put', choices=('add', 'stupid', 'update', 'copy'), default='update', metavar='MODE',
            help='set put mode (add, stupid, copy or update)')
    group.add_option('--prefix', default='', metavar='PREFIX',
            help='set key prefix')
    group.add_option('--resume', action='append', default=[], metavar='FILENAME',
            help='resume from log file')
    group.add_option('--grant', metavar='GRANT', default=CannedACLStrings[DEFAULT_CANNED_ACL], choices=CannedACLStrings,
            help='A canned ACL policy to be applied to each file uploaded.\nChoices: %s' %
            ', '.join(CannedACLStrings))
    group.add_option('--header', metavar='HEADER:VALUE', dest='headers', action='append',
                     help='extra headers to add to the file, can be specified multiple times')
    group.add_option('--encrypt-key', action='store_true', default=False, dest='encrypt_key',
            help='use server side encryption')
    parser.add_option_group(group)
    group = OptionGroup(parser, 'Logging options')
    group.add_option('--log-filename', metavar='FILENAME',
            help='set log filename')
    group.add_option('--quiet', '-q', action='count', default=0,
            help='less output')
    group.add_option('--verbose', '-v', action='count', default=0,
            help='more output')
    parser.add_option_group(group)
    group = OptionGroup(parser, 'yas3fs options')
    group.add_option('--yas3fs', action='store_true', default=False,
            help='If this bucket is intended to be used with yas3fs, write yas3fs metadata attributes')
    group.add_option('--yas3fs-uid', metavar='UID', type=int, default=0,
            help='UID to be placed in the yas3fs metadata')
    group.add_option('--yas3fs-gid', metavar='GID', type=int, default=0,
            help='GID to be placed in the yas3fs metadata')
    parser.add_option_group(group)
    group = OptionGroup(parser, 'Debug and performance tuning options')
    group.add_option('--dry-run', action='store_true',
            help='don\'t write to S3')
    group.add_option('--limit', metavar='N', type=int,
            help='set maximum number of keys to put')
    group.add_option('--processes', default=8, metavar='PROCESSES', type=int,
            help='set number of putter processes')
    parser.add_option_group(group)
    options, args = parser.parse_args(argv[1:])
    logging.basicConfig(filename=options.log_filename, level=logging.INFO + 10 * (options.quiet - options.verbose))
    logger = logging.getLogger(os.path.basename(sys.argv[0]))
    if len(args) < 1:
        logger.error('missing source operand')
        return 1
    if not options.bucket:
        logger.error('missing bucket')
        return 1
    if not options.bucket_region:
        options.bucket_region = 'us-east-1'
    start = time.time()
    put_queue = JoinableQueue(1024 * options.processes)
    stat_queue = JoinableQueue()
    walk = {'filesystem': walk_filesystem, 'tar': walk_tar}[options.walk]
    walker_process = Process(target=walker, args=(walk, put_queue, args, options))
    walker_process.start()
    put = {'add': put_add, 'stupid': put_stupid, 'update': put_update, 'copy': put_copy}[options.put]
    putter_processes = list(islice(repeatedly(Process, target=putter, args=(put, put_queue, stat_queue, options)), options.processes))
    for putter_process in putter_processes:
        putter_process.start()
    statter_process = Process(target=statter, args=(stat_queue, start, options))
    statter_process.start()
    walker_process.join()
    for putter_process in putter_processes:
        put_queue.put(None)
    put_queue.close()
    for putter_process in putter_processes:
        putter_process.join()
    stat_queue.put(None)
    stat_queue.close()
    statter_process.join()
    put_queue.join_thread()
    stat_queue.join_thread()


if __name__ == '__main__':
    sys.exit(main(sys.argv))
